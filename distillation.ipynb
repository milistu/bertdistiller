{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from src.minilm.config import DataArguments, DataConfig, DatasetSource, ModelConfig\n",
    "from src.minilm.data import get_tokenized_datasets\n",
    "from src.minilm.trainer import DistillationConfig, DistillationTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets  Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_source_1 = DatasetSource(\n",
    "    name=\"bookcorpus/bookcorpus\", column=\"text\", is_hf=True\n",
    ")\n",
    "# dataset_source_2 = DatasetSource(\n",
    "#     name=\"legacy-datasets/wikipedia\",\n",
    "#     subset=\"20220301.en\",\n",
    "#     column=\"text\",\n",
    "#     is_hf=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(sources=[dataset_source_1], cache_dir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = DataArguments(train_config=data_config, max_seq_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    input_model_dir=\"google-bert/bert-base-uncased\",\n",
    "    student_hidden_size=768,\n",
    "    student_num_layers=6,\n",
    "    student_attention_heads=12,\n",
    "    teacher_layer=12,  # for all models from paper\n",
    "    num_relation_heads=48,  # 48 for base models and 64 for large models from paper\n",
    "    model_type=\"bert\",\n",
    "    cache_dir=\".cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('results/google-bert-bert-base-uncased_2025-Feb-10_09-51-42')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = datetime.now().strftime(\"%Y-%b-%d_%H-%M-%S\")\n",
    "output_dir = Path(\"results\") / (\n",
    "    model_config.input_model_dir.replace(\"/\", \"-\") + \"_\" + dt\n",
    ")\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=6e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_steps=400_000,\n",
    "    warmup_steps=4_000,\n",
    "    logging_steps=100,  # 1_000,\n",
    "    save_steps=500,  # 50_000,\n",
    "    seed=42,\n",
    "    ddp_find_unused_parameters=True,\n",
    "    save_total_limit=5,\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    save_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistillationConfig(\n",
    "    model_config=model_config,\n",
    "    training_args=training_args,\n",
    "    data_args=data_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-10 09:58:56.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.minilm.trainer.distillation\u001b[0m:\u001b[36m_create_teacher\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mLoading teacher model...\u001b[0m\n",
      "\u001b[32m2025-02-10 09:58:56.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.minilm.trainer.distillation\u001b[0m:\u001b[36m_create_student\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mCreating student model...\u001b[0m\n",
      "\u001b[32m2025-02-10 09:58:57.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.minilm.trainer.distillation\u001b[0m:\u001b[36m_create_student\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mStudent configuration:\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[32m2025-02-10 09:58:58.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.minilm.trainer.distillation\u001b[0m:\u001b[36m_create_distiller\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mInitializing MiniLM distiller...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = DistillationTrainer(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2de84690704ce1b65229d82b3db18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing bookcorpus/bookcorpus (num_proc=11):   0%|          | 0/74004228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-10 09:37:33.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.minilm.data.utils\u001b[0m:\u001b[36mprepare_dataset\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mCreated dataset with 74004228 samples from 1 sources\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset = get_tokenized_datasets(\n",
    "    data_args=data_args,\n",
    "    tokenizer=trainer.tokenizer,\n",
    "    tokenization_kwargs={\"padding\": \"do_not_pad\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-10 09:59:02.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.minilm.trainer.distillation\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mStarting training...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3729' max='400000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3729/400000 1:03:05 < 111:47:44, 0.98 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.771100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.569700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.551400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.539800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.519100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.507900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.495100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.546800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/minilmv2/src/minilm/trainer/distillation.py:159\u001b[0m, in \u001b[0;36mDistillationTrainer.train\u001b[0;34m(self, train_dataset, eval_dataset)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m    158\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Development/minilmv2/.venv/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/minilmv2/.venv/lib/python3.10/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Development/minilmv2/.venv/lib/python3.10/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m~/Development/minilmv2/.venv/lib/python3.10/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Development/minilmv2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/minilmv2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Development/minilmv2/src/minilm/models/minilm.py:321\u001b[0m, in \u001b[0;36mMiniLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m    318\u001b[0m s_vectors \u001b[38;5;241m=\u001b[39m [s_query, s_key, s_value]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Compute scaled dot-product attention\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m teacher_sim \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -1 because relations are 1-based\u001b[39;49;00m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(teacher_head_size)\n\u001b[1;32m    326\u001b[0m student_sim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[1;32m    327\u001b[0m     s_vectors[m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    328\u001b[0m     s_vectors[n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    329\u001b[0m ) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(student_head_size)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Calculate KL divergence loss\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the minilmv2 repository. I want to recreate it in my repo but with improvements. \n",
    "The idea is to go over the code step by step and do code refactoring, optimization, pythonic approach, and generally make the code better and more understandable. This will help me get to know the code because I am new to it. \n",
    "When we have a scope of the whole project, we will start to improve the whole structure. Here is the link to the repository: https://github.com/LazarusNLP/minilmv2.bb It is a fork of the original but I think it has some better improvements so that is why I want to use both the fork version and original (https://github.com/bloomberg/minilmv2.bb). \n",
    "I want to create a library from this and the documentation, but I think that it is best to leave it for the finish. \n",
    "I already created a repo, cloned it and implement most of the scripts. This is my project structure:\n",
    "minilmv2-py3.10➜  minilmv2 git:(feat/train) ✗ tree\n",
    ".\n",
    "├── README.md\n",
    "├── configs\n",
    "│   ├── default\n",
    "│   │   ├── train_config.json\n",
    "│   │   └── val_config.json\n",
    "│   └── examples\n",
    "│       ├── custom_dataset.json\n",
    "│       ├── mixed_sources.json\n",
    "│       └── wikipedia_books.json\n",
    "├── distillation.ipynb\n",
    "├── docs\n",
    "│   └── index.md\n",
    "├── examples\n",
    "│   ├── dataset_basic_usage.py\n",
    "│   └── distillation_usage.py\n",
    "├── poetry.lock\n",
    "├── pyproject.toml\n",
    "├── src\n",
    "│   └── minilm\n",
    "│       ├── __init__.py\n",
    "│       ├── __pycache__\n",
    "│       │   └── __init__.cpython-310.pyc\n",
    "│       ├── config\n",
    "│       │   ├── __init__.py\n",
    "│       │   ├── __pycache__\n",
    "│       │   │   ├── __init__.cpython-310.pyc\n",
    "│       │   │   ├── data_config.cpython-310.pyc\n",
    "│       │   │   ├── model_config.cpython-310.pyc\n",
    "│       │   │   ├── parsers.cpython-310.pyc\n",
    "│       │   │   ├── parsers_old.cpython-310.pyc\n",
    "│       │   │   └── training_config.cpython-310.pyc\n",
    "│       │   ├── data_config.py\n",
    "│       │   ├── model_config.py\n",
    "│       │   ├── parsers.py\n",
    "│       │   └── training_config.py\n",
    "│       ├── data\n",
    "│       │   ├── __init__.py\n",
    "│       │   ├── __pycache__\n",
    "│       │   │   ├── __init__.cpython-310.pyc\n",
    "│       │   │   ├── data_utils.cpython-310.pyc\n",
    "│       │   │   ├── data_utils_org.cpython-310.pyc\n",
    "│       │   │   └── utils.cpython-310.pyc\n",
    "│       │   └── utils.py\n",
    "│       ├── models\n",
    "│       │   ├── __init__.py\n",
    "│       │   ├── __pycache__\n",
    "│       │   │   ├── __init__.cpython-310.pyc\n",
    "│       │   │   ├── minilm.cpython-310.pyc\n",
    "│       │   │   └── minilmv2_old.cpython-310.pyc\n",
    "│       │   ├── minilm.py\n",
    "│       │   └── minilmv2_old.py\n",
    "│       ├── scripts\n",
    "│       │   └── run_distillation.py\n",
    "│       ├── trainer\n",
    "│       │   ├── __init__.py\n",
    "│       │   ├── __pycache__\n",
    "│       │   │   ├── __init__.cpython-310.pyc\n",
    "│       │   │   └── distillation.cpython-310.pyc\n",
    "│       │   └── distillation.py\n",
    "│       └── utils\n",
    "├── test.ipynb\n",
    "├── test_output\n",
    "│   ├── checkpoint-100\n",
    "│   │   ├── model.safetensors\n",
    "│   │   ├── optimizer.pt\n",
    "│   │   ├── rng_state.pth\n",
    "│   │   ├── scheduler.pt\n",
    "│   │   ├── trainer_state.json\n",
    "│   │   └── training_args.bin\n",
    "│   ├── checkpoint-125\n",
    "│   │   ├── model.safetensors\n",
    "│   │   ├── optimizer.pt\n",
    "│   │   ├── rng_state.pth\n",
    "│   │   ├── scheduler.pt\n",
    "│   │   ├── trainer_state.json\n",
    "│   │   └── training_args.bin\n",
    "│   ├── checkpoint-25\n",
    "│   │   ├── model.safetensors\n",
    "│   │   ├── optimizer.pt\n",
    "│   │   ├── rng_state.pth\n",
    "│   │   ├── scheduler.pt\n",
    "│   │   ├── trainer_state.json\n",
    "│   │   └── training_args.bin\n",
    "│   └── checkpoint-50\n",
    "│       ├── model.safetensors\n",
    "│       ├── optimizer.pt\n",
    "│       ├── rng_state.pth\n",
    "│       ├── scheduler.pt\n",
    "│       ├── trainer_state.json\n",
    "│       └── training_args.bin\n",
    "└── tests\n",
    "    └── __init__.py\n",
    "\n",
    "25 directories, 68 files\n",
    "\n",
    "Questions:\n",
    "1. I want to test my project and see if I am getting the same or close results as they got. Here is their results that they mentioned in readme:\n",
    "|                      | qnli  | qqp   | rte   | sst2  | mnli  | Avg    |\n",
    "|----------------------|-------|-------|-------|-------|-------|--------|\n",
    "| MiniLM 6x768 (ours)  | 89.05 | 90.47 | 60.65 | 91.63 | 82.92 |  82.94 |\n",
    "| MiniLM 6x384 (ours)  | 89.44 | 90.47 | 63.18 | 91.28 | 82.59 | 83.392 |\n",
    "| MiniLM 6x768 (paper) |  90.8 |  91.1 |  72.1 |  92.4 |  84.2 |  86.12 |\n",
    "| MiniLM 6x384 (paper) | 90.24 | 90.51 | 66.43 | 91.17 | 82.91 |  84.25 |\n",
    "\n",
    "And here is the command that they left as how to start the training run:\n",
    "CMD=\"python -m minilmv2.run_distillation -- \"\n",
    "# Set seed to ensure reproducibility. Set to -1 for no seed\n",
    "SEED=42\n",
    "\n",
    "ARGS=\"data_params \\\n",
    "    --train_config ./train_config.json \\\n",
    "    training_params \\\n",
    "      --per_device_train_batch_size 256 \\\n",
    "      --learning_rate 6e-4 \\\n",
    "      --adam_epsilon 1e-6 \\\n",
    "      --adam_beta1 0.9 \\\n",
    "      --adam_beta2 0.999 \\\n",
    "      --weight_decay 0.01 \\\n",
    "      --max_steps 400000 \\\n",
    "      --save_steps 50000 \\\n",
    "      --logging_steps 1000 \\\n",
    "      --warmup_steps 4000 \\\n",
    "    --ddp_find_unused_parameters  true\\\n",
    "    --output_dir ./out\\\n",
    "    --seed=${SEED}\\\n",
    "    model_params \\\n",
    "     --input_model_dir ./model/bert-base-uncased/ \\\n",
    "      --student_hidden_size 384 \\\n",
    "      --student_num_layers 6 \\\n",
    "      --student_attention_heads 12 \\\n",
    "      --L 12 \\\n",
    "      --num_relation_heads 48\\\n",
    "\"\n",
    "\n",
    "$CMD $ARGS\n",
    "My question is for what model are these commands? I do not understand the naming of their models. `MiniLM 6x768` what is 6 and 768? How should I set my parameters to get both of their models?\n",
    "\n",
    "2. I want to get the model similar to their two model architectures but using different model as a teacher. I want to use ModernBERT. I will give you the config of both original BERT model and new one ModernBERt and you tell me how to set the parameters so I get same models as they from results? Because architectures are different do I need to change something?\n",
    "\n",
    "\n",
    "* google-bert/bert-base-uncased\n",
    "* Config:\n",
    "BertConfig {\n",
    "  \"_attn_implementation_autoset\": true,\n",
    "  \"_name_or_path\": \"bert-base-uncased\",\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.48.2\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 30522\n",
    "}\n",
    "* Arhitecture:\n",
    "<bound method Module.parameters of BertModel(\n",
    "  (embeddings): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "    (position_embeddings): Embedding(512, 768)\n",
    "    (token_type_embeddings): Embedding(2, 768)\n",
    "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (encoder): BertEncoder(\n",
    "    (layer): ModuleList(\n",
    "      (0-11): 12 x BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSdpaSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (pooler): BertPooler(\n",
    "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "    (activation): Tanh()\n",
    "  )\n",
    ")>\n",
    "\n",
    "But it does not work for new ModerBERT for some reason:\n",
    "* model: answerdotai/ModernBERT-base\n",
    "* Config:\n",
    "ModernBertConfig {\n",
    "  \"_attn_implementation_autoset\": true,\n",
    "  \"_name_or_path\": \"answerdotai/ModernBERT-base\",\n",
    "  \"architectures\": [\n",
    "    \"ModernBertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_bias\": false,\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 50281,\n",
    "  \"classifier_activation\": \"gelu\",\n",
    "  \"classifier_bias\": false,\n",
    "  \"classifier_dropout\": 0.0,\n",
    "  \"classifier_pooling\": \"mean\",\n",
    "  \"cls_token_id\": 50281,\n",
    "  \"decoder_bias\": true,\n",
    "  \"deterministic_flash_attn\": false,\n",
    "  \"embedding_dropout\": 0.0,\n",
    "  \"eos_token_id\": 50282,\n",
    "  \"global_attn_every_n_layers\": 3,\n",
    "  \"global_rope_theta\": 160000.0,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_activation\": \"gelu\",\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_cutoff_factor\": 2.0,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 1152,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"local_attention\": 128,\n",
    "  \"local_rope_theta\": 10000.0,\n",
    "  \"max_position_embeddings\": 8192,\n",
    "  \"mlp_bias\": false,\n",
    "  \"mlp_dropout\": 0.0,\n",
    "  \"model_type\": \"modernbert\",\n",
    "  \"norm_bias\": false,\n",
    "  \"norm_eps\": 1e-05,\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 22,\n",
    "  \"pad_token_id\": 50283,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"reference_compile\": null,\n",
    "  \"repad_logits_with_grad\": false,\n",
    "  \"sep_token_id\": 50282,\n",
    "  \"sparse_pred_ignore_index\": -100,\n",
    "  \"sparse_prediction\": false,\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.48.2\",\n",
    "  \"vocab_size\": 50368\n",
    "}\n",
    "* Arhitecture:\n",
    "<bound method Module.parameters of ModernBertModel(\n",
    "  (embeddings): ModernBertEmbeddings(\n",
    "    (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
    "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "    (drop): Dropout(p=0.0, inplace=False)\n",
    "  )\n",
    "  (layers): ModuleList(\n",
    "    (0): ModernBertEncoderLayer(\n",
    "      (attn_norm): Identity()\n",
    "      (attn): ModernBertAttention(\n",
    "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
    "        (rotary_emb): ModernBertRotaryEmbedding()\n",
    "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
    "        (out_drop): Identity()\n",
    "      )\n",
    "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "      (mlp): ModernBertMLP(\n",
    "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
    "        (act): GELUActivation()\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1-21): 21 x ModernBertEncoderLayer(\n",
    "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "      (attn): ModernBertAttention(\n",
    "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
    "        (rotary_emb): ModernBertRotaryEmbedding()\n",
    "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
    "        (out_drop): Identity()\n",
    "      )\n",
    "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "      (mlp): ModernBertMLP(\n",
    "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
    "        (act): GELUActivation()\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    ")>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
